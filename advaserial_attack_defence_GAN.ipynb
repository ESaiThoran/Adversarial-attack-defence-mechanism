{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ANAplEzfKaY"
   },
   "source": [
    "**Installing and Importing libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 701,
     "status": "ok",
     "timestamp": 1616934726175,
     "user": {
      "displayName": "Anirudh Yadav",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh8WICwfod4NhGebrsu6kLpJBB1-8ly7K-2pCTWlA=s64",
      "userId": "07973118493860517555"
     },
     "user_tz": -330
    },
    "id": "mshbDZRpFZ9C",
    "outputId": "9ad0c7b6-ca98-48d6-e71e-8444dd8e1790"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/tensorflow/cleverhans.git#egg=cleverhans\n",
    "#!git clone https://github.com/EliSchwartz/imagenet-sample-images.git imagenet\n",
    "#!mkdir data\n",
    "#!mv imagenet data/\n",
    "!mkdir extra\n",
    "!pip install opencv-python\n",
    "!pip install tqdm\n",
    "!pip uninstall torch torchvision\n",
    "!pip install torch torchvision\n",
    "!pip install --no-deps torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T17:30:56.518856Z",
     "start_time": "2019-06-21T17:30:56.432867Z"
    },
    "hidden": true,
    "id": "ANW46hypWBQF",
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "import cv2\n",
    "import pdb\n",
    "import os\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import keras\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import keras.layers as layers\n",
    "import keras.models as models\n",
    "import matplotlib.pylab as plt\n",
    "from keras import backend as K\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.initializers import orthogonal\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from cleverhans.tf2.attacks.fast_gradient_method import fast_gradient_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctBDIG-1naPg"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%matplotlib inline\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qH4Sa6hrfUVv"
   },
   "source": [
    "Unzipping Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sn0dKX47TAEz"
   },
   "outputs": [],
   "source": [
    "!unzip test.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzFhwtmBfXkx"
   },
   "source": [
    "Importing Model and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2524,
     "status": "ok",
     "timestamp": 1616934776473,
     "user": {
      "displayName": "Anirudh Yadav",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh8WICwfod4NhGebrsu6kLpJBB1-8ly7K-2pCTWlA=s64",
      "userId": "07973118493860517555"
     },
     "user_tz": -330
    },
    "id": "UXX17wCA8gEJ",
    "outputId": "2fb9632a-9dc2-447d-810b-e1b0cda96d0a"
   },
   "outputs": [],
   "source": [
    "pretrained_model = tf.keras.applications.MobileNetV2(include_top=True,weights='imagenet')\n",
    "pretrained_model.trainable = False\n",
    "decode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions\n",
    "\n",
    "  \n",
    "def get_imagenet_label(probs):\n",
    "  return decode_predictions(probs, top=1)[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T17:30:58.722277Z",
     "start_time": "2019-06-21T17:30:58.648226Z"
    },
    "hidden": true,
    "id": "AjyCnxReWBQP"
   },
   "outputs": [],
   "source": [
    "batch_size =1\n",
    "epochs = 150\n",
    "input_shape = (224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkIB-X_JfgwV"
   },
   "source": [
    "Setting Autoencoder layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T17:31:04.968915Z",
     "start_time": "2019-06-21T17:31:04.872818Z"
    },
    "hidden": true,
    "id": "sfXwNeaRWBQp"
   },
   "outputs": [],
   "source": [
    "def Conv2DLayer(x, filters, kernel, strides, padding, block_id, kernel_init=orthogonal()):\n",
    "    prefix = f'block_{block_id}_'\n",
    "    x = layers.Conv2D(filters, kernel_size=kernel, strides=strides, padding=padding,kernel_initializer=kernel_init, name=prefix+'conv')(x)\n",
    "    x = layers.LeakyReLU(name=prefix+'lrelu')(x)\n",
    "    x = layers.Dropout(0.2, name=prefix+'drop')((x))\n",
    "    x = layers.BatchNormalization(name=prefix+'conv_bn')(x)\n",
    "    return x\n",
    "\n",
    "def Transpose_Conv2D(x, filters, kernel, strides, padding, block_id, kernel_init=orthogonal()):\n",
    "    prefix = f'block_{block_id}_'\n",
    "    x = layers.Conv2DTranspose(filters, kernel_size=kernel, strides=strides, padding=padding,kernel_initializer=kernel_init, name=prefix+'de-conv')(x)\n",
    "    x = layers.LeakyReLU(name=prefix+'lrelu')(x)\n",
    "    x = layers.Dropout(0.2, name=prefix+'drop')((x))\n",
    "    x = layers.BatchNormalization(name=prefix+'conv_bn')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def AutoEncdoer(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    conv1 = Conv2DLayer(inputs, 64, 3, strides=1, padding='same', block_id=1)\n",
    "    conv2 = Conv2DLayer(conv1, 64, 3, strides=2, padding='same', block_id=2)\n",
    "    conv3 = Conv2DLayer(conv2, 128, 5, strides=2, padding='same', block_id=3)\n",
    "    conv4 = Conv2DLayer(conv3, 128, 3, strides=1, padding='same', block_id=4)\n",
    "    conv5 = Conv2DLayer(conv4, 256, 5, strides=2, padding='same', block_id=5)\n",
    "    conv6 = Conv2DLayer(conv5, 512, 3, strides=2, padding='same', block_id=6)\n",
    "    conv7 = Conv2DLayer(conv6, 1024, 3, strides=2, padding='same', block_id=60)\n",
    "    deconv0 = Transpose_Conv2D(conv7, 1024, 3, strides=2, padding='same', block_id=7)\n",
    "    skip0 = layers.concatenate([deconv0, conv6], name='skip0')\n",
    "    conv77 = Conv2DLayer(skip0, 512, 3, strides=1, padding='same', block_id=80)\n",
    "    deconv1 = Transpose_Conv2D(conv77, 256, 3, strides=2, padding='same', block_id=90)\n",
    "\n",
    "    skip1 = layers.concatenate([deconv1, conv5], name='skip1')\n",
    "    conv7 = Conv2DLayer(skip1, 256, 3, strides=1, padding='same', block_id=8)\n",
    "    deconv2 = Transpose_Conv2D(conv7, 128, 3, strides=2, padding='same', block_id=9)\n",
    "    skip2 = layers.concatenate([deconv2, conv3], name='skip2')\n",
    "    conv8 = Conv2DLayer(skip2, 128, 5, strides=1, padding='same', block_id=10)\n",
    "    deconv3 = Transpose_Conv2D(conv8, 64, 3, strides=2, padding='same', block_id=11)\n",
    "    skip3 = layers.concatenate([deconv3, conv2], name='skip3')\n",
    "    conv9 = Conv2DLayer(skip3, 64, 5, strides=1, padding='same', block_id=12)\n",
    "    deconv4 = Transpose_Conv2D(conv9, 64, 3, strides=2, padding='same', block_id=13)\n",
    "    skip3 = layers.concatenate([deconv4, conv1])\n",
    "    conv10 = layers.Conv2D(3, 3, strides=1, padding='same', activation='sigmoid',kernel_initializer=orthogonal(), name='final_conv')(skip3)\n",
    "    return models.Model(inputs=inputs, outputs=conv10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "OIEKXsASWBQv"
   },
   "outputs": [],
   "source": [
    "model = AutoEncdoer((*input_shape, 3))\n",
    "model_opt = Adam(lr=0.002)\n",
    "model.compile(optimizer=model_opt, loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUOwyXu3fjRe"
   },
   "source": [
    "**Importing Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1399,
     "status": "ok",
     "timestamp": 1616937939131,
     "user": {
      "displayName": "Anirudh Yadav",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh8WICwfod4NhGebrsu6kLpJBB1-8ly7K-2pCTWlA=s64",
      "userId": "07973118493860517555"
     },
     "user_tz": -330
    },
    "id": "Q-MzcynriqTf",
    "outputId": "199857a4-2b28-473a-e934-f5ad28676ec0"
   },
   "outputs": [],
   "source": [
    "all_images = []\n",
    "from keras.preprocessing import image\n",
    "error = 0\n",
    "face_images = glob.glob('dataset/*')\n",
    "for i in tqdm(face_images):\n",
    "  img = image.load_img(i, target_size=(224,224,3))\n",
    "  img = image.img_to_array(img)\n",
    "  img = img/255.\n",
    "  all_images.append(img)\n",
    "print(\"Total Faulty Images \",error)\n",
    "all_images = np.array(all_images)\n",
    "train_x, val_x = train_test_split(all_images, random_state=32, test_size=0.4 , shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJUvP46Nf7qC"
   },
   "source": [
    "Setting up attack variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obTRIV3ykl-m"
   },
   "outputs": [],
   "source": [
    "from cleverhans.tf2.attacks import fast_gradient_method\n",
    "from cleverhans.tf2.attacks.fast_gradient_method import fast_gradient_method\n",
    "logits_model = tf.keras.Model(pretrained_model.input,pretrained_model.layers[-1].output)\n",
    "\n",
    "def attack(image):\n",
    "  epsilon = 0.1\n",
    "  adv_example_untargeted_label = fast_gradient_method(logits_model, image, 0.1 , np.inf, targeted=False)\n",
    "  adv_example_untargeted_label_pred = pretrained_model.predict((adv_example_untargeted_label))\n",
    "  return adv_example_untargeted_label[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xk4z7gzLgE_8"
   },
   "source": [
    "Creating Training and testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eP3MY7wQlK7e"
   },
   "outputs": [],
   "source": [
    "train_x_px = []\n",
    "val_x_px = []\n",
    "\n",
    "for i in range(train_x.shape[0]):\n",
    "  temp = attack(train_x[None,i,:,:,:])\n",
    "  train_x_px.append(temp)\n",
    "\n",
    "for i in range(val_x.shape[0]):\n",
    "  temp = attack(val_x[None,i,:,:,:])\n",
    "  val_x_px.append(temp)\n",
    "\n",
    "train_x_px = np.array(train_x_px)\n",
    "val_x_px = np.array(val_x_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 461494,
     "status": "ok",
     "timestamp": 1616765890356,
     "user": {
      "displayName": "Anirudh Yadav",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh8WICwfod4NhGebrsu6kLpJBB1-8ly7K-2pCTWlA=s64",
      "userId": "07973118493860517555"
     },
     "user_tz": -330
    },
    "id": "VwrbOq0rv_HR",
    "outputId": "6146d158-5c11-40ad-a49d-b2a491633b43"
   },
   "outputs": [],
   "source": [
    "early_stopper = EarlyStopping(monitor='val_loss', mode='auto')\n",
    "\n",
    "model.fit(train_x_px, train_x,\n",
    "            epochs=20,\n",
    "            batch_size=25,\n",
    "            validation_data=(val_x_px, val_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WOcJtYyTIyb0"
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"okish_2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWyxFwZQgNzL"
   },
   "source": [
    "Downloading saved weights from google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30468,
     "status": "ok",
     "timestamp": 1616934887766,
     "user": {
      "displayName": "Anirudh Yadav",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh8WICwfod4NhGebrsu6kLpJBB1-8ly7K-2pCTWlA=s64",
      "userId": "07973118493860517555"
     },
     "user_tz": -330
    },
    "id": "POE1ptviBAAa",
    "outputId": "4b95da7c-376f-4413-c746-d34df05d3cb4"
   },
   "outputs": [],
   "source": [
    "!gdown --id 1_i2tl6_BiFBGAMrlH5kqeKNAyagwmvkE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecJQnebXgQ_E"
   },
   "source": [
    "Loading Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-enxigiXAQ4q"
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"okish_2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXKjXnBwgTIt"
   },
   "source": [
    "Output of Autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "executionInfo": {
     "elapsed": 3044,
     "status": "ok",
     "timestamp": 1616941799874,
     "user": {
      "displayName": "Anirudh Yadav",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh8WICwfod4NhGebrsu6kLpJBB1-8ly7K-2pCTWlA=s64",
      "userId": "07973118493860517555"
     },
     "user_tz": -330
    },
    "id": "2mLWc10dx4Af",
    "outputId": "3b4687bb-7273-491a-a968-9123d2269758"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(val_x_px)\n",
    "\n",
    "n = 5\n",
    "plt.figure(figsize= (20,10))\n",
    "\n",
    "for i in range(n):\n",
    "  ax = plt.subplot(2, n, i+1)\n",
    "  plt.imshow(val_x_px[i+25])\n",
    "  ax.get_xaxis().set_visible(False)\n",
    "  ax.get_yaxis().set_visible(False)\n",
    "\n",
    "  ax = plt.subplot(2, n, i+1+n)\n",
    "  plt.imshow(predictions[i+25])\n",
    "  ax.get_xaxis().set_visible(False)\n",
    "  ax.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jl8yTRxgrMvm"
   },
   "source": [
    "Bringing randomization in our classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7362,
     "status": "ok",
     "timestamp": 1616941126411,
     "user": {
      "displayName": "Anirudh Yadav",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh8WICwfod4NhGebrsu6kLpJBB1-8ly7K-2pCTWlA=s64",
      "userId": "07973118493860517555"
     },
     "user_tz": -330
    },
    "id": "1dAvm1zHSbcD",
    "outputId": "0739b050-38a1-4a33-d52a-e5c7f54f9a6f"
   },
   "outputs": [],
   "source": [
    "model_id = 2\n",
    "if model_id == 1:\n",
    "    squeezenet = models.squeezenet1_1(pretrained=True)\n",
    "elif model_id == 2:\n",
    "    resnet = ResNet50(weights='imagenet')\n",
    "elif model_id == 3:\n",
    "    densenet = models.densenet161(pretrained=True)\n",
    "\n",
    "for layer in resnet.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "random_noise = [0.001,0.0001,0.0002,0.0003,0.0004,0.0005,0.0006,0.0007,0.0008,0.0009]\n",
    "\n",
    "before = []\n",
    "for i in range(10):\n",
    "  image = val_x[i][None,:,:,:]\n",
    "  original = Image.fromarray((image[0]*255).astype(np.uint8),\"RGB\")\n",
    "  original.save('extra/original.jpg')\n",
    "  _, classes_auto, probs = get_imagenet_label(resnet.predict((cv2.imread('extra/original.jpg')[None,:,:,:])))\n",
    "  before.append((classes_auto, probs))\n",
    "\n",
    "for i in range(50,100):\n",
    "  tem = resnet.layers[i].get_weights()\n",
    "  for j in range(len(tem)):\n",
    "    tem[j] += np.average(tem[j])*random.choice(random_noise)\n",
    "  resnet.layers[i].set_weights(tem)\n",
    "\n",
    "after = []\n",
    "\n",
    "for i in range(10):\n",
    "  image = val_x[i][None,:,:,:]\n",
    "  original = Image.fromarray((image[0]*255).astype(np.uint8),\"RGB\")\n",
    "  original.save('extra/original.jpg')\n",
    "  _, classes_auto, probs = get_imagenet_label(resnet.predict((cv2.imread('extra/original.jpg')[None,:,:,:])))\n",
    "  after.append((classes_auto, probs))\n",
    "\n",
    "for i in range(len(before)):\n",
    "  print(\"Image :\",i)\n",
    "  print(before[i],after[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "3b6ee0e8021149859e482c1cb325e02a",
      "ba205c8fc4e342f1887565b1aefbdaef",
      "d96ab64088df4df49d76749d17c68573",
      "fcbeec19fe294b2b8f4682fe7e45439d",
      "997934705ac14b47a3020ad12065870b",
      "b2e5190373284b2ba727c7aaf91cad7c",
      "d7e42a8a88ad4418a75cf53f65424ca4",
      "5655b5df1efc4af28f29da04d365d41b"
     ]
    },
    "executionInfo": {
     "elapsed": 3185,
     "status": "ok",
     "timestamp": 1616937277818,
     "user": {
      "displayName": "Anirudh Yadav",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh8WICwfod4NhGebrsu6kLpJBB1-8ly7K-2pCTWlA=s64",
      "userId": "07973118493860517555"
     },
     "user_tz": -330
    },
    "id": "gqrFLozWnne3",
    "outputId": "7af56c6c-d64f-4d63-b627-2fa70f140575"
   },
   "outputs": [],
   "source": [
    "from torchvision import models, transforms\n",
    "LABELS_URL = 'https://s3.amazonaws.com/outcome-blog/imagenet/labels.json'\n",
    "model_id = 2\n",
    "if model_id == 1:\n",
    "    net = models.squeezenet1_1(pretrained=True)\n",
    "    finalconv_name = 'features' # this is the last conv layer of the network\n",
    "elif model_id == 2:\n",
    "    net = models.resnet50(pretrained=True)\n",
    "    finalconv_name = 'layer4'\n",
    "elif model_id == 3:\n",
    "    net = models.densenet161(pretrained=True)\n",
    "    finalconv_name = 'features'\n",
    "\n",
    "net.eval()\n",
    "\n",
    "features_blobs = []\n",
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.data.cpu().numpy())\n",
    "\n",
    "net._modules.get(finalconv_name).register_forward_hook(hook_feature)\n",
    "\n",
    "params = list(net.parameters())\n",
    "weight_softmax = np.squeeze(params[-2].data.numpy())\n",
    "\n",
    "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "    size_upsample = (224, 224)\n",
    "    bz, nc, h, w = feature_conv.shape\n",
    "    output_cam = []\n",
    "    for idx in class_idx:\n",
    "        cam = weight_softmax[idx].dot(feature_conv.reshape((nc, h*w)))\n",
    "        cam = cam.reshape(h, w)\n",
    "        cam = cam - np.min(cam)\n",
    "        cam_img = cam / np.max(cam)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "    return output_cam\n",
    "normalize = transforms.Normalize(\n",
    "   mean=[0.485, 0.456, 0.406],\n",
    "   std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "preprocess = transforms.Compose([\n",
    "   transforms.Resize((224,224)),\n",
    "   transforms.ToTensor(),\n",
    "   normalize\n",
    "])\n",
    "\n",
    "classes = {int(key):value for (key, value)\n",
    "          in requests.get(LABELS_URL).json().items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orLVfc0Upk73"
   },
   "source": [
    "Displaying some Examples with Autoencoder output and Grad-CAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 35695,
     "status": "ok",
     "timestamp": 1616941426172,
     "user": {
      "displayName": "Anirudh Yadav",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh8WICwfod4NhGebrsu6kLpJBB1-8ly7K-2pCTWlA=s64",
      "userId": "07973118493860517555"
     },
     "user_tz": -330
    },
    "id": "ydUyeGtqicKp",
    "outputId": "3da51a3c-ac1f-4c78-df7a-bcf5e780fe96"
   },
   "outputs": [],
   "source": [
    "for i in range(50,70):\n",
    "  image = all_images[i][None,:,:,:]\n",
    "  original = Image.fromarray((image[0]*255).astype(np.uint8),\"RGB\")\n",
    "  original.save('extra/original.jpg')\n",
    "  attack_img = fast_gradient_method(logits_model, image, 0.1 , np.inf, targeted=False)\n",
    "  tf.keras.preprocessing.image.save_img('extra/attack.jpg',attack_img[0])\n",
    "  auto_enc = model.predict(attack_img)\n",
    "  im = Image.fromarray((auto_enc[0]*255).astype(np.uint8),\"RGB\")\n",
    "  im.save('extra/auto_enc.jpg')\n",
    "\n",
    "  predc = []\n",
    "\n",
    "  img_tensor = preprocess(original)\n",
    "  img_variable = Variable(img_tensor.unsqueeze(0))\n",
    "  logit = net(img_variable)\n",
    "  h_x = F.softmax(logit, dim=1).data.squeeze()\n",
    "  probs, idx = h_x.sort(0, True)\n",
    "  probs = probs.numpy()\n",
    "  idx = idx.numpy()\n",
    "  predc.append('{:.3f} -> {}'.format(probs[0], classes[idx[0]]))\n",
    "\n",
    "  _, classes_auto, probs = get_imagenet_label(pretrained_model.predict(attack_img))\n",
    "  predc.append('{:.3f} -> {}'.format(probs, classes_auto))\n",
    "\n",
    "  _, classes_auto, probs = get_imagenet_label(resnet.predict((cv2.imread('extra/auto_enc.jpg')[None,:,:,:])))\n",
    "  predc.append('{:.3f} -> {}'.format(probs, classes_auto))\n",
    "\n",
    "  CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0]])\n",
    "\n",
    "  #print('output CAM.jpg for the top1 prediction: %s'%classes[idx[0]])\n",
    "  img = cv2.imread('extra/attack.jpg')\n",
    "  height, width, _ = img.shape\n",
    "  heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
    "  result = heatmap * 0.3 + img * 0.5\n",
    "  cv2.imwrite('extra/CAM_attack.jpg', result)\n",
    "\n",
    "  img = cv2.imread('extra/original.jpg')\n",
    "  height, width, _ = img.shape\n",
    "  heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
    "  result = heatmap * 0.3 + img * 0.5\n",
    "  cv2.imwrite('extra/CAM_original.jpg', result)\n",
    "\n",
    "  img = cv2.imread(\"extra/auto_enc.jpg\")\n",
    "  height, width, _ = img.shape\n",
    "  heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
    "  result = heatmap * 0.3 + img * 0.5\n",
    "  cv2.imwrite('extra/CAM_au.jpg', result)\n",
    "\n",
    "  fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "  fig.add_subplot(1, 4, 1)\n",
    "  plt.imshow((image[0] * 255).astype(np.uint8))\n",
    "  plt.axis('off')\n",
    "  plt.title(predc[0])\n",
    "\n",
    "  fig.add_subplot(1, 4, 2)\n",
    "  plt.imshow(Image.open('extra/attack.jpg'))\n",
    "  plt.axis('off')\n",
    "  plt.title(predc[1])\n",
    "\n",
    "  fig.add_subplot(1, 4, 3)\n",
    "  plt.imshow(im)\n",
    "  plt.axis('off')\n",
    "  plt.title(predc[2])\n",
    "\n",
    "  fig.add_subplot(1, 4, 4)\n",
    "  plt.imshow(Image.open('extra/CAM_au.jpg'))\n",
    "  plt.axis('off')\n",
    "  plt.title(predc[2])\n",
    "\n",
    "  #fig.add_subplot(1, 5, 5)\n",
    "  #plt.imshow(Image.open('extra/CAM_attack.jpg'))\n",
    "  #plt.axis('off')\n",
    "  #plt.title(predc[1])\n",
    "\n",
    "  fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 97201,
     "status": "ok",
     "timestamp": 1616941037095,
     "user": {
      "displayName": "Anirudh Yadav",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh8WICwfod4NhGebrsu6kLpJBB1-8ly7K-2pCTWlA=s64",
      "userId": "07973118493860517555"
     },
     "user_tz": -330
    },
    "id": "mkS6CCQhwFIl",
    "outputId": "200fb131-c951-4638-f8e3-dad28fc9a718"
   },
   "outputs": [],
   "source": [
    "attack_success = 0\n",
    "attack_failed = 0\n",
    "defense_success = 0\n",
    "defense_failed = 0\n",
    "epsilon = 0.01\n",
    "resnet = ResNet50(weights='imagenet')\n",
    "for layer in resnet.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "for i in range(len(val_x)):\n",
    "  image = val_x[i][None,:,:,:]\n",
    "  adv_example_untargeted_label = fast_gradient_method(logits_model, image, 0.1 , np.inf, targeted=False)\n",
    "  adv_example_untargeted_label_pred = pretrained_model.predict((adv_example_untargeted_label))\n",
    "  _, image_class_at, class_confidence_at = get_imagenet_label(adv_example_untargeted_label_pred)\n",
    "  #print('Attack : {} : {:.2f}% Confidence'.format(image_class_at, class_confidence_at*100) , end = \"\\t\\t\")\n",
    "  image_probs = pretrained_model.predict((image))\n",
    "  _, image_class_o, class_confidence_o = get_imagenet_label(image_probs)\n",
    "  #print('Original : {} : {:.2f}% Confidence'.format(image_class_o, class_confidence_o*100) , end = \"\\t\\t\")\n",
    "  \n",
    "  im = Image.fromarray((model.predict(adv_example_untargeted_label)[0]*255).astype(np.uint8),\"RGB\")\n",
    "  im.save('extra/attack.jpg')\n",
    "  _, image_class_au, class_confidence_au = get_imagenet_label(resnet.predict((cv2.imread('extra/attack.jpg')[None,:,:,:])))\n",
    "  #print('Auto : {} : {:.2f}% Confidence'.format(image_class_au, class_confidence_au*100))\n",
    "\n",
    "  if image_class_au != image_class_o:\n",
    "    defense_failed += 1\n",
    "    #print('Original : {} : {:.2f}% Confidence'.format(image_class_o, class_confidence_o*100) , end = \"\\t\\t\")\n",
    "    #print('Attack : {} : {:.2f}% Confidence'.format(image_class_at, class_confidence_at*100) , end = \"\\t\\t\")\n",
    "    #print('Auto : {} : {:.2f}% Confidence'.format(image_class_au, class_confidence_au*100),i)\n",
    "  else:\n",
    "    defense_success += 1\n",
    "  if image_class_at == image_class_o:\n",
    "    attack_failed += 1\n",
    "  else:\n",
    "    attack_success += 1\n",
    "#print(attack_success,attack_failed,defense_success,defense_failed)\n",
    "\n",
    "print(\"-+---------------------------------------------+-\")\n",
    "print(\" |  Attack Succesfully Performed in :\",attack_success,\"images|\")\n",
    "print(\" |  Attack failed in :\",attack_failed,\"images                |\")\n",
    "print(\"-+---------------------------------------------+-\")\n",
    "print(\" |  Defense succesfull in :\",defense_success,\"images          |\")\n",
    "print(\" |  Defense failed in :\",defense_failed,\"images              |\")\n",
    "print(\"-+---------------------------------------------+-\")\n",
    "print(\" |  Attack Model Accuracy : {:0.2f}%             |\".format(attack_success))\n",
    "print(\" |  Defense Model Accuracy : {:0.2f}%            |\".format(defense_success/len(val_x)*100))\n",
    "print(\"-+---------------------------------------------+-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ITHUVn67ywtP"
   },
   "outputs": [],
   "source": [
    "|95 8 84 19\n",
    "\n",
    "125 15 102 38\n",
    "\n",
    "EXTRA\n",
    "\n",
    "for i in range(len(good)):\n",
    "  img_pil = Image.fromarray((good[i]*255).astype(np.uint8),\"RGB\")\n",
    "  img_pil.save('dataset/{}.jpg'.format(i+239))\n",
    "\n",
    "!kaggle datasets download -d mikewallace250/tiny-imagenet-challenge\n",
    "!unzip \\*.zip  && rm *.zip\n",
    "\n",
    "def display_images(image):\n",
    "  #_, label, confidence = get_imagenet_label(pretrained_model.predict(image))\n",
    "  plt.figure()\n",
    "  plt.imshow(image)\n",
    "  #plt.title('{} \\n {} : {:.2f}% Confidence'.format(description,                                                   label, confidence*100))\n",
    "  plt.show()\n",
    "\n",
    "def preprocess(image):\n",
    "  image = tf.cast(image, tf.float32)\n",
    "  image = tf.image.resize(image, (224, 224))\n",
    "  image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n",
    "  image = image[None, ...]\n",
    "  return image\n",
    "\n",
    "\n",
    "P_MODELSAVE = 'saved_models'\n",
    "P_LOGS = 'logs'\n",
    "P_IMGSAVE = 'saved_images'\n",
    "\n",
    "dirs = [P_MODELSAVE, P_LOGS, P_IMGSAVE]\n",
    "\n",
    "for d in dirs:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "\n",
    "         count = 0\n",
    "failed = 0\n",
    "saved = 0\n",
    "correct = []\n",
    "#pretrained_model = pred\n",
    "from cleverhans.tf2.attacks.fast_gradient_method import fast_gradient_method\n",
    "logits_model = tf.keras.Model(pred.input,pred.layers[-1].output)\n",
    "epsilon = 0.01\n",
    "for i in range(999):\n",
    "  image = all_images[None,i,:,:,:]\n",
    "  image_probs = pretrained_model.predict((image))\n",
    "  _, image_class_o, class_confidence_o = get_imagenet_label(image_probs)\n",
    "  \n",
    "  im = Image.fromarray((image[0]*255).astype(np.uint8),\"RGB\")\n",
    "  im.save('test.png')\n",
    "  _, image_class_au, class_confidence_au = get_imagenet_label(resnet.predict((cv2.imread('test.png')[None,:,:,:])))\n",
    "  \n",
    "\n",
    "  \n",
    "  if image_class_au == image_class_o:\n",
    "    #print('Original : {} : {:.2f}% Confidence'.format(image_class_o, class_confidence_o*100) , end = \"\\t\\t\")\n",
    "    #print('Orig_res : {} : {:.2f}% Confidence'.format(image_class_au, class_confidence_au*100))\n",
    "    count+=1\n",
    "    correct.append(image[0])\n",
    "print(count)\n",
    "\n",
    "\n",
    "\n",
    "count = 0\n",
    "failed = 0\n",
    "saved = 0\n",
    "#pretrained_model = pred\n",
    "\n",
    "logits_model = tf.keras.Model(pred.input,pred.layers[-1].output)\n",
    "epsilon = 0.01\n",
    "actual = []\n",
    "for i in range(200):\n",
    "  image = all_images[None,i,:,:,:]\n",
    "  adv_example_untargeted_label = fast_gradient_method(logits_model, image, 0.1 , np.inf, targeted=False)\n",
    "  adv_example_untargeted_label_pred = pretrained_model.predict((adv_example_untargeted_label))\n",
    "  _, image_class_at, class_confidence_at = get_imagenet_label(adv_example_untargeted_label_pred)\n",
    "  #print('Attack : {} : {:.2f}% Confidence'.format(image_class_at, class_confidence_at*100) , end = \"\\t\\t\")\n",
    "  image_probs = pretrained_model.predict((image))\n",
    "  _, image_class_o, class_confidence_o = get_imagenet_label(image_probs)\n",
    "  #print('Original : {} : {:.2f}% Confidence'.format(image_class_o, class_confidence_o*100) , end = \"\\t\\t\")\n",
    "  #tt = model.predict(adv_example_untargeted_label)\n",
    "  #image_probs = pretrained_model.predict(tt)\n",
    "  #_, image_class_au, class_confidence_au = get_imagenet_label(image_probs)\n",
    "  \n",
    "  im = Image.fromarray((model.predict(adv_example_untargeted_label)[0]*255).astype(np.uint8),\"RGB\")\n",
    "  im.save('test.png')\n",
    "  _, image_class_au, class_confidence_au = get_imagenet_label(resnet.predict((cv2.imread('test.png')[None,:,:,:])))\n",
    "  #print('Auto : {} : {:.2f}% Confidence'.format(image_class_au, class_confidence_au*100))\n",
    "  if image_class_au == image_class_o :\n",
    "    count+=1\n",
    "  else:\n",
    "    failed+=1\n",
    "  if class_confidence_au < 0.4 and class_confidence_o > 0.8:\n",
    "    failed += 1\n",
    "    #print('Same',i) \n",
    "    #print(\"Saved\" , class_confidence_au , class_confidence_at , class_confidence_o)\n",
    "print(count , saved , failed)\n",
    "\n",
    "correct = []\n",
    "for i in range(999):\n",
    "  image = all_images[None,i,:,:,:]\n",
    "  image_probs = pretrained_model.predict((image))\n",
    "  _, image_class_o, class_confidence_o = get_imagenet_label(image_probs)\n",
    "  im = Image.fromarray((image[0]*255).astype(np.uint8),\"RGB\")\n",
    "  im.save('test.png')\n",
    "  _, image_class_au, class_confidence_au = get_imagenet_label(resnet.predict((cv2.imread('test.png')[None,:,:,:])))\n",
    "  if image_class_au == image_class_o:\n",
    "    count+=1\n",
    "    correct.append(image[0])\n",
    "print(count)\n",
    "\n",
    "\n",
    "for i in range(len(correct)):\n",
    "  image = correct[i][None,:,:,:]A\n",
    "  im = Image.fromarray((image[0]*255).astype(np.uint8),\"RGB\")\n",
    "  im.save('Sample_data/{}.jpg'.format(i))\n",
    "\n",
    "  \n",
    "!zip -r /content/data.zip /content/Sample_data\n",
    "\n",
    "\n",
    "tt = model.predict(adv_example_untargeted_label)\n",
    "image_probs = pretrained_model.predict(tt)\n",
    "plt.figure()\n",
    "display_images(tt[0])\n",
    "_, image_class, class_confidence = get_imagenet_label(image_probs)\n",
    "print('{} : {:.2f}% Confidence'.format(image_class, class_confidence*100))\n",
    "\n",
    "\n",
    "\n",
    "t = []\n",
    "for i in range(20):\n",
    "  image = train_x[i][None,:,:,:]\n",
    "  adv_example_untargeted_label = fast_gradient_method(logits_model, image, 0.1 , np.inf, targeted=False)\n",
    "  adv_example_untargeted_label_pred = pretrained_model.predict((adv_example_untargeted_label))\n",
    "  _, image_class_at, class_confidence_at = get_imagenet_label(adv_example_untargeted_label_pred)  \n",
    "  #tf.keras.preprocessing.image.save_img('test.png',image[0])\n",
    "  im = Image.fromarray((model.predict(adv_example_untargeted_label)[0]*255).astype(np.uint8),\"RGB\")\n",
    "  im.save('test.png')\n",
    "  _, image_class_au, class_confidence_au = get_imagenet_label(resnet.predict((cv2.imread('test.png')[None,:,:,:])))\n",
    "  #plt.imshow(adv_example_untargeted_label[0])\n",
    "  #plt.show()\n",
    "  print('Auto : {} : {:.2f}% Confidence'.format(image_class_au, class_confidence_au*100))\n",
    "  t.append(image_class_au)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TVq01-1OtzNj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Copy of Major Project_IMPLE.ipynb",
   "provenance": [
    {
     "file_id": "1ZtmpUcovP_H1a3MgW15j-u3kP2ADB945",
     "timestamp": 1617722335637
    },
    {
     "file_id": "1nX0KiGl0W2YQsVqh7nvTU5yotd-rkFVP",
     "timestamp": 1616942666645
    },
    {
     "file_id": "https://github.com/nsarang/ImageDenoisingAutoencdoer/blob/master/DenoisingAutoencoder.ipynb",
     "timestamp": 1616703529397
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "3b6ee0e8021149859e482c1cb325e02a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d96ab64088df4df49d76749d17c68573",
       "IPY_MODEL_fcbeec19fe294b2b8f4682fe7e45439d"
      ],
      "layout": "IPY_MODEL_ba205c8fc4e342f1887565b1aefbdaef"
     }
    },
    "5655b5df1efc4af28f29da04d365d41b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "997934705ac14b47a3020ad12065870b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "b2e5190373284b2ba727c7aaf91cad7c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba205c8fc4e342f1887565b1aefbdaef": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7e42a8a88ad4418a75cf53f65424ca4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d96ab64088df4df49d76749d17c68573": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b2e5190373284b2ba727c7aaf91cad7c",
      "max": 102502400,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_997934705ac14b47a3020ad12065870b",
      "value": 102502400
     }
    },
    "fcbeec19fe294b2b8f4682fe7e45439d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5655b5df1efc4af28f29da04d365d41b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d7e42a8a88ad4418a75cf53f65424ca4",
      "value": " 97.8M/97.8M [00:04&lt;00:00, 21.6MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
